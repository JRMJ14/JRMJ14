{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZr3GKjnrYg9jOs6N+JhoB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JRMJ14/JRMJ14/blob/main/clima.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exWN7F3Rsxsk",
        "outputId": "b1933171-34ef-441f-bf01-12be09867551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rutas de navegacion**\n",
        "\n",
        "juego donde Puedes implementar un entorno que simule diferentes condiciones climáticas donde el agente deba tomar decisiones sobre cómo navegar, por ejemplo, rutas de navegación por aire o mar que eviten condiciones peligrosas."
      ],
      "metadata": {
        "id": "_BWrLVeB-5_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importamos las respectivas librerias necesarias para el desarrollo del entorno\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "CvsglqtCth24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NavigationEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(NavigationEnv, self).__init__()\n",
        "\n",
        "        # Tamaño del entorno de navegación (10x10)\n",
        "        self.grid_size = 10\n",
        "\n",
        "        # Espacio de observación: todas las posiciones posibles (10x10 = 100 estados)\n",
        "        self.observation_space = gym.spaces.Discrete(self.grid_size ** 2)\n",
        "\n",
        "        # Espacio de acción: arriba, abajo, izquierda, derecha\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "\n",
        "        # Definir punto de inicio y destino\n",
        "        self.start_position = (0, 0)\n",
        "        self.goal_position = (9, 9)\n",
        "\n",
        "        # Zonas de clima adverso (ubicaciones peligrosas)\n",
        "        self.bad_weather_zones = [(3, 3), (4, 5), (6, 6), (7, 2), (8, 8)]\n",
        "\n",
        "        # Estado inicial del agente\n",
        "        self.state = self.start_position\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.state = self.start_position  # Reiniciar el agente al inicio\n",
        "        return self._get_state_index(self.state), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.state\n",
        "\n",
        "        # Movimiento basado en la acción seleccionada\n",
        "        if action == 0 and y > 0:          # Arriba\n",
        "            y -= 1\n",
        "        elif action == 1 and y < self.grid_size - 1:  # Abajo\n",
        "            y += 1\n",
        "        elif action == 2 and x > 0:        # Izquierda\n",
        "            x -= 1\n",
        "        elif action == 3 and x < self.grid_size - 1:  # Derecha\n",
        "            x += 1\n",
        "\n",
        "        # Actualizar el estado del agente\n",
        "        self.state = (x, y)\n",
        "\n",
        "        # Recompensas y finalización\n",
        "        reward = -1  # Penalización por cada paso\n",
        "        done = False\n",
        "\n",
        "        # Si alcanza el destino, gran recompensa\n",
        "        if self.state == self.goal_position:\n",
        "            reward = 20\n",
        "            done = True\n",
        "        # Penalización adicional por entrar en clima peligroso\n",
        "        elif self.state in self.bad_weather_zones:\n",
        "            reward = -10\n",
        "\n",
        "        return self._get_state_index(self.state), reward, done, False, {}\n",
        "\n",
        "    def _get_state_index(self, position):\n",
        "        \"\"\"Convierte una posición (x, y) a un índice único para la Q-table.\"\"\"\n",
        "        return position[0] * self.grid_size + position[1]"
      ],
      "metadata": {
        "id": "8-tGGqFotlJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el entorno\n",
        "env = NavigationEnv()"
      ],
      "metadata": {
        "id": "C170E5-Ott3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parámetros de aprendizaje Q-Learning\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])  # Tabla Q inicial\n",
        "alpha = 0.1       # Tasa de aprendizaje\n",
        "gamma = 0.9       # Factor de descuento\n",
        "epsilon = 1.0     # Factor de exploración inicial\n",
        "epsilon_decay = 0.995  # Decaimiento de epsilon para reducir la exploración\n",
        "min_epsilon = 0.01"
      ],
      "metadata": {
        "id": "K_gqyJJ-uZWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento del agente\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Elegir acción (exploración vs explotación)\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()  # Exploración\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])  # Explotación\n",
        "\n",
        "        # Ejecutar la acción en el entorno\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "        # Actualizar la Q-table\n",
        "        best_next_action = np.argmax(q_table[next_state])\n",
        "        td_target = reward + gamma * q_table[next_state, best_next_action]\n",
        "        td_error = td_target - q_table[state, action]\n",
        "        q_table[state, action] += alpha * td_error\n",
        "\n",
        "        # Avanzar al siguiente estado\n",
        "        state = next_state\n",
        "\n",
        "    # Decaer epsilon después de cada episodio\n",
        "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "\n",
        "print(\"Entrenamiento completado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwC876Q2ubcE",
        "outputId": "538761aa-ea6e-4b97-a2f4-f548b651d329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenamiento completado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluación del agente\n",
        "test_episodes = 10\n",
        "for episode in range(test_episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])  # Elegir la mejor acción\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "    print(f\"Recompensa total en episodio de prueba {episode + 1}: {total_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEjH0ZHHueZr",
        "outputId": "7d2dbf49-f502-4758-ba39-67c1cf96c901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recompensa total en episodio de prueba 1: 3\n",
            "Recompensa total en episodio de prueba 2: 3\n",
            "Recompensa total en episodio de prueba 3: 3\n",
            "Recompensa total en episodio de prueba 4: 3\n",
            "Recompensa total en episodio de prueba 5: 3\n",
            "Recompensa total en episodio de prueba 6: 3\n",
            "Recompensa total en episodio de prueba 7: 3\n",
            "Recompensa total en episodio de prueba 8: 3\n",
            "Recompensa total en episodio de prueba 9: 3\n",
            "Recompensa total en episodio de prueba 10: 3\n"
          ]
        }
      ]
    }
  ]
}